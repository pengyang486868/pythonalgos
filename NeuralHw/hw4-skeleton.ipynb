{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4\n",
    "## Following are skeleton code for hw4, no numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000,) (25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feel free to write other helper functions\n",
    "And change the parameters/return values of the function\n",
    "To make them work better\n",
    "For the first part, you can't use numpy or similar ml libraries but can use utilities such as math\n",
    "You can also write a class about neural network and put related functions inside it\n",
    "for loading data still feel free to use numpy\n",
    "To make things easier, your data can be in numpy form, but \n",
    "in the first part you should not use numpy functionality such as matmul or dot or related matrix/vectorization methods\n",
    "in the second part feel free to use anything in numpy\n",
    "\n",
    "EXTREMELY IMPORTANT THINGS:\n",
    "1) do -1 on y to get the correct labels\n",
    "2) the first value in the weights theta_1 and theta_2 files is the bias\n",
    "\n",
    "for the skeleton code you don't have to follow it, they are here only for your reference\n",
    "but if you have no idea where to start you can use them to guide your way\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time \n",
    "import math\n",
    "# read in data and weights\n",
    "# since label data is in fact 1-10, we will change them into 0 to 9\n",
    "X = np.genfromtxt('ps5_data.csv',delimiter=',')\n",
    "y = np.genfromtxt('ps5_data-labels.csv',delimiter=',')\n",
    "y = y-1\n",
    "W1 = np.genfromtxt('ps5_theta1.csv',delimiter=',')\n",
    "W2 = np.genfromtxt('ps5_theta2.csv',delimiter=',')\n",
    "print(X.shape, y.shape, W1.shape, W2.shape)\n",
    "\n",
    "## 4.2 softmax and sigmoid activation\n",
    "def softmax(yhat):\n",
    "    # 4.2\n",
    "\n",
    "    return result\n",
    "def sigmoid(z):\n",
    "    \n",
    "    return \n",
    "\n",
    "def toCategorical(y):\n",
    "    # convert to one hot\n",
    "\n",
    "    return y_one_hot\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.W1 = None #W1 should be 401 * 25\n",
    "        self.W2 = None #W2 should be 26 * 10\n",
    "    def setWeight(self, W1, W2):\n",
    "        \n",
    "    def neuronActivation(self,Aprev,W,with_sigmoid=True):\n",
    "        # 4.1\n",
    "        #this will take activation of previous layer and the weight of previous layer to this layer\n",
    "        #to produce a single activation value\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def final_layer(self, activations):\n",
    "        # 4.3 activation of the final layer\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # 4.4 \n",
    "        #non-vectorized. Given 1 example, return a prediction\n",
    "\n",
    "        return probabilities\n",
    "    def classify(self,X): \n",
    "        # 4.5, should use 4.4 forward function\n",
    "        # will classify an image and return a number\n",
    "\n",
    "        return int(predicted_label)\n",
    "    def get_error_rate(self, X, y):\n",
    "        # 4.6 will give error rate on given X, y\n",
    "        # you can use python's time\n",
    "        start_time = time.time()\n",
    "\n",
    "        return error_rate, end_time-start_time\n",
    "    def giveCost(self, X, y):\n",
    "        # 4.7\n",
    "        # gives the value of the cross entropy cost function\n",
    "\n",
    "        return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the program and report error rate, time used for classify all X, and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024800000000000044 0.15284346245189467 13.00907301902771\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.setWeight(W1.transpose(), W2.transpose())\n",
    "error_rate, time_used = nn.get_error_rate(X, y)\n",
    "cost = nn.giveCost(X, y)\n",
    "print(error_rate, cost, time_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rate, cost and time used should be similar to:\n",
    "\n",
    "0.0248 0.1528 12.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following are skeleton code for hw4, numpy vectorized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000,) (25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time \n",
    "# read in data and weights\n",
    "# since label data is in fact 1-10, we will change them into 0 to 9\n",
    "\n",
    "X = np.genfromtxt('ps5_data.csv',delimiter=',')\n",
    "y = np.genfromtxt('ps5_data-labels.csv',delimiter=',')\n",
    "y = y-1\n",
    "W1 = np.genfromtxt('ps5_theta1.csv',delimiter=',')\n",
    "W2 = np.genfromtxt('ps5_theta2.csv',delimiter=',')\n",
    "print(X.shape, y.shape, W1.shape, W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2 softmax and sigmoid activation\n",
    "def softmax(X, dim):\n",
    "    return \n",
    "def sigmoid(X):\n",
    "    return \n",
    "def toCategorical(y):\n",
    "    # convert to one hot\n",
    "    \n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.W1 = None #W1 should be 401 * 25\n",
    "        self.W2 = None #W2 should be 26 * 10\n",
    "    def setWeight(self, W1, W2):\n",
    "\n",
    "    def AddBias(self, X):\n",
    "        #will add the bias term to the input matrix\n",
    "\n",
    "        return Xnew\n",
    "    def hidden_layer_activation(self, X, W):\n",
    "        ## 4.1\n",
    "        ## use sigmoid activation for hidden layers\n",
    "\n",
    "        return \n",
    "    def final_layer_activation(self, X, W):\n",
    "        ## 4.3\n",
    "        ## only applies to final layer\n",
    "        ## note you only use softmax, don't use sigmoid here\n",
    "\n",
    "        return \n",
    "    def forward(self, X):\n",
    "        ## 4.4, should use 4.1, 4.3 functions\n",
    "\n",
    "        return probabilities \n",
    "    def classify(self,X): \n",
    "        # 4.5, should use 4.4 forward function\n",
    "        # will classify an image and return a number\n",
    "\n",
    "        return \n",
    "    def get_error_rate(self, X, y):\n",
    "        # 4.6 will give error rate on given X, y\n",
    "        start_time = time.time()\n",
    "\n",
    "        return error_rate, end_time-start_time\n",
    "    def giveCost(self, X, y):\n",
    "        # 4.7\n",
    "        # gives the value of the cross entropy cost function\n",
    "\n",
    "        return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the program and report error rate, time used for classify all X, and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024800000000000044 0.152843462451895 0.020461082458496094\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.setWeight(W1.transpose(), W2.transpose())\n",
    "error_rate, time_used = nn.get_error_rate(X, y)\n",
    "cost = nn.giveCost(X, y)\n",
    "print(error_rate, cost, time_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rate, cost and time used should be similar to:\n",
    "\n",
    "0.0248 0.1528 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is vectorization making things faster?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl] *",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
