{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus land\n",
    "\n",
    "Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this [dataset](dinos.txt). (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs' wrath! \n",
    "\n",
    "By completing this assignment you will learn:\n",
    "\n",
    "- How to store text data for processing using an RNN \n",
    "- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit\n",
    "- How to build a character-level text generation recurrent neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# torch.set_printoptions(linewidth=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# hidden_size = 100\n",
    "class DinosDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('G:\\pythonalgos\\RNNhw\\dinos.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content))\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.lines = content.splitlines()\n",
    "\n",
    "        self.ch_to_idx = {}\n",
    "        self.idx_to_ch = {}\n",
    "\n",
    "        self.ch_to_idx['\\n'] = 0\n",
    "        self.idx_to_ch[0] = '\\n'\n",
    "\n",
    "        for i in range(26):\n",
    "            curch = chr(i + 97)\n",
    "            self.ch_to_idx[curch] = i + 1\n",
    "            self.idx_to_ch[i + 1] = curch\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        x_str = ' ' + line  # add a space at the beginning, which indicates a vector of zeros.\n",
    "        y_str = line + '\\n'\n",
    "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
    "        y = torch.empty(len(x_str), dtype=torch.long)\n",
    "\n",
    "        y[0] = self.ch_to_idx[y_str[0]]\n",
    "        #we start from the second character because the first character of x was nothing(vector of zeros).\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str[1:], y_str[1:]), 1):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\npamparaptor\n1536\n27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          1., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 1., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          1., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([16,  1, 13, 16,  1, 18,  1, 16, 20, 15, 18,  0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do some test on the dataset\n",
    "dset_test = DinosDataset()\n",
    "print(dset_test.ch_to_idx)\n",
    "print(dset_test.lines[1016])\n",
    "print(len(dset_test))\n",
    "print(dset_test.vocab_size)\n",
    "dset_test[1016]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, inputsize):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=inputsize,\n",
    "            hidden_size=inputsize,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        return r_out, h_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,3,3,3,3,2,3,3,0,2,2,2,3,3,2,2,0,2,3,3,"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do some test on random choice API\n",
    "prob = np.array([0.1, 0, 0.4, 0.5])\n",
    "\n",
    "for i in range(20):\n",
    "    smplindx = np.random.choice(range(len(prob)), p=prob.ravel())\n",
    "    print(smplindx, end=\",\")\n",
    "    \n",
    "tensortest=torch.Tensor([1,2,3]).float()\n",
    "np.array(tensortest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sample(model: RNN):\n",
    "    # my comment: notify in eval mode now\n",
    "    model.eval()\n",
    "\n",
    "    word_size = 0\n",
    "    newline_idx = trn_ds.ch_to_idx['\\n']\n",
    "    indices = []\n",
    "    pred_char_idx = -1\n",
    "\n",
    "    # Step 1: initialize first input and hidden state ---all zero---\n",
    "    h_prev = torch.zeros([1, 1, trn_ds.vocab_size], dtype=torch.float)\n",
    "    x = torch.zeros([1, 1, trn_ds.vocab_size], dtype=torch.float)\n",
    "\n",
    "    # my comment: with torch.no_grad() is to stop autograd to speed up a bit\n",
    "    with torch.no_grad():\n",
    "        while pred_char_idx != newline_idx and word_size != 50:\n",
    "            # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "            x, h_prev = model.forward(x, h_prev)\n",
    "\n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            # Step 3: Sample the index of a character within the vocabulary from the probability distribution\n",
    "            # print(x[0, 0].sum())\n",
    "            idx = np.random.choice(range(x.size(2)), p=np.array(F.softmax(x[0, 0])).ravel())\n",
    "            indices.append(idx)\n",
    "\n",
    "            # Step 4: Overwrite the input character as the one corresponding to the sampled index.            \n",
    "            x = torch.zeros([1, 1, trn_ds.vocab_size], dtype=torch.float)\n",
    "            x[0, 0, idx] = 1\n",
    "            pred_char_idx = idx\n",
    "            word_size += 1\n",
    "        if word_size == 50:\n",
    "            indices.append(newline_idx)\n",
    "    return indices\n",
    "\n",
    "def print_sample(sample_idxs):\n",
    "    print(trn_ds.idx_to_ch[sample_idxs[0]].upper(), end='')\n",
    "    [print(trn_ds.idx_to_ch[x], end='') for x in sample_idxs[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model: RNN, loss_fn, optimizer):\n",
    "    # Go through the training examples one at a time\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        # my comment: it should means switching to training mode\n",
    "        model.train()\n",
    "\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize parameters\n",
    "        h_prev = torch.zeros([1, 1, trn_ds.vocab_size], dtype=torch.float)\n",
    "        #outval, h_prev = model.forward(x, h_prev)\n",
    "\n",
    "        # y_correct = torch.empty([1,x.shape[1],trn_ds.vocab_size], dtype=torch.long)\n",
    "        for i in range(x.shape[1]):\n",
    "            # Forward propagate through the RNN to compute the loss\n",
    "            outval, h_prev = model.forward(x[:, i:i + 1, :], h_prev)\n",
    "            #h_prev=h_prev.data\n",
    "            y_correct = y[:, i]\n",
    "            loss += loss_fn(outval[:, 0, :], y_correct)\n",
    "\n",
    "        # Backpropagate through time\n",
    "        # loss /= x.shape[1]\n",
    "        # print(h_prev)\n",
    "        #loss = loss_fn(h_prev[:, 0, :], y[:, -1])\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip your gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Every 100 steps of stochastic gradient descent, \n",
    "        # print one sampled name to see how the algorithm is doing\n",
    "        if (line_num + 1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "            print('loss=', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "def train(trn_ds, trn_dl, epochs=1):\n",
    "    model = RNN(trn_ds.vocab_size)\n",
    "    \n",
    "    # Use cross entropy loss\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use Adam\n",
    "    learning_rate = 0.02\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        print(f'{\"-\" * 20} Epoch {e} {\"-\" * 20}')\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\pythonalgos\\venv\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metim\nloss= tensor(30.3124, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talxovattuqbsskzsissu\nloss= tensor(30.1973, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jmams\nloss= tensor(30.9522, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sfvadrn\nloss= tensor(40.0185, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lethiatanxrwatstlgsqnztmssv\nloss= tensor(21.1736, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klans\nloss= tensor(24.8296, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sevaern\nloss= tensor(43.2001, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lbsematanxnwauutlgspmzsftqu\nloss= tensor(36.2747, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jlaiuauhvacpo\nloss= tensor(42.0481, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leshiatanxowatssifsqozsgtsu\nloss= tensor(19.6102, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inantateuaesk\nloss= tensor(37.7732, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lethiatanxowatsshestiyshqsvaniajrateuaati\nloss= tensor(41.0390, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lerikatanxowatushersizsgtru\nloss= tensor(36.5629, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilaltasbuagro\nloss= tensor(42.1001, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesfiatapxnwatssgispiysgrru\nloss= tensor(35.8453, grad_fn=<AddBackward0>)\n-------------------- Epoch 2 --------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jialsaukvafur\nloss= tensor(26.0146, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesdiauamxnvatruoisqkzsitptankakuauhvaeup\nloss= tensor(28.7146, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lerhiatanxqwdtutmisrozshqsv\nloss= tensor(39.1976, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imaluateuafrn\nloss= tensor(23.7043, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kerffauanxmvatsrlesqiytisru\nloss= tensor(38.5819, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hlaluatfuagrk\nloss= tensor(33.3610, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesamatanxnwasuuqgushysgrsvanjajuauivafur\nloss= tensor(24.2650, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leriiauanyswatusicosezsgrqu\nloss= tensor(32.5329, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiagrathvaern\nloss= tensor(28.4321, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesbiaualxnwdrutmispizshpsvanmamtbsawafrn\nloss= tensor(35.5858, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesbiauanxowatstkeronysfrqu\nloss= tensor(28.4290, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hlaltasawafrk\nloss= tensor(29.9542, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesalas\nloss= tensor(23.1902, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hxpufutrolsroztiruv\nloss= tensor(32.0262, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiais\nloss= tensor(27.9004, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "train(trn_ds, trn_dl, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl]",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
